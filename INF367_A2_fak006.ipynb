{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF367 Assignment 2, Exercise 2\n",
    "Author: Johanna JÃ¸sang (fak006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import Image\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "n = 10 #number of variables\n",
    "delta = 0.1\n",
    "eps = 0.4\n",
    "eta = 0.01\n",
    "\n",
    "ten_div_delta = int(10/delta)\n",
    "k = 1000\n",
    "N = 40*k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes about methods used for PAC implementation with noise\n",
    "The target is generated in the exact same manner as the previous assignment, so the description of it is the same.\n",
    "Since a hypothesis only can be given positive examples if the target is satisfiable, only satisfiable targets are used in this implementation. A satisfiable target is generated by combining data generated using a normal distribution. \"1\" indicates that a variable is evaluated to True, \"0\" that is is evaluated to False. The target does not necessarily contain all literals, hence it may contain \"3\" which indicated that this variable is not present in the target. \\\n",
    "Target generation example:\\\n",
    "[0, 1, 1, 0, 0, 0, 1, 1, 0, 0] <- basis 1 and \\\n",
    "[0, 1, 1, 0, 1, 0, 0, 0, 0, 1] <- basis 2 generate:\\\n",
    "[0, 1, 1, 0, 3, 0, 3, 3, 0, 3] <- target\n",
    "\n",
    "Examples are generated in the same way as bases for the target, by normal distribution. Each example is categorized by whether it satisfies the target or not. With a probability of eta, the examples will be misclassified. The following examples are based on the target given above. Notice that the valuation of a variable not present in the target has no influence on the categorization.\n",
    "\n",
    "([0, 1, 1, 1, 0, 0, 1, 1, 0, 1],  0) <- valuation that doesn't satisfy target \\\n",
    "([0, 1, 1, 0, 1, 0, 1, 0, 0, 0],  1) <- valuation that does satisfy target \\\n",
    "([0, 1, 0, 1, 0, 0, 1, 1, 0, 1],  1) <- valuation that doesn't satisfy target, but is misclassified \n",
    "\n",
    "Since the algorithm for learning with noise needs to keep track of statistical information for each literal, the hypothesis is represented slightly differently than in my previous assignment. Each literal gets it's own position in the array, so that information about it, such as the p0-value, can be stored there.\n",
    "\n",
    "![title](literals_representation.png)\n",
    "\n",
    "After training, the hypothesis would look something like this:\n",
    "[0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1], where each \"1\" represents the presence of a certian literal. So if we translate this to the normal representation of literals, we get:\n",
    "[0, 1, 1, 0, 3, 0, 1, 0, 0, 0]\n",
    "\n",
    "If we compare the target and the final hypothesis we can see the result. \\\n",
    "Target-----: [0, 1, 1, 0, 3, 0, 3, 3, 0, 3] \\\n",
    "Trained hyp: [0, 1, 1, 0, 3, 0, 1, 0, 0, 0] \n",
    "\n",
    "The hypothesis has not quite reached the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uncategorized_data(size, dimensions):\n",
    "    \"\"\"\n",
    "    Using a random normal distribution, 2D datasets of 1s and 0s are generated\n",
    "    \n",
    "    :param size: number of arrays to generate\n",
    "    :param dimensions: length of each array\n",
    "    :return : 2D array of uncategorized data\n",
    "    \n",
    "    \"\"\"\n",
    "    normal_distribution = np.random.normal(size=[size, dimensions]) >= 0\n",
    "    normal_distribution_ints = normal_distribution.astype(int)                        \n",
    "    return normal_distribution_ints.tolist()\n",
    "\n",
    "def generate_satisfiable_target(size, dimensions):\n",
    "    \"\"\"\n",
    "    Using a random normal distribution, 2D datasets of 1s and 0s are generated\n",
    "    \n",
    "    :param size: number of arrays to generate\n",
    "    :param dimensions: length of each array\n",
    "    \n",
    "    \"\"\"\n",
    "    target_basis_ints = generate_uncategorized_data(size, dimensions)\n",
    "    target = [1]*(len(target_basis_ints[0]))\n",
    "    for j in range (len(target_basis_ints[0])):\n",
    "        column_target_value = 3\n",
    "        column_bool = target_basis_ints[0][j]\n",
    "        use_bool = True\n",
    "        for i in range (len(target_basis_ints)):\n",
    "            if (target_basis_ints[i][j] != column_bool):\n",
    "                column_target_value = 3\n",
    "                use_bool = False\n",
    "                continue\n",
    "        if(use_bool):\n",
    "            if(column_bool == True):\n",
    "                column_target_value = 1\n",
    "            else:\n",
    "                column_target_value = 0\n",
    "        target[j] = column_target_value#\n",
    "    return target\n",
    "\n",
    "def categorize_data(valuation, target):\n",
    "    \"\"\"\n",
    "    Categorizes valuations based on if they satisfy the target   \n",
    "    \n",
    "    :param valuation: valuation to categorize\n",
    "    :param target: used classify the valuation\n",
    "    :return : 1 if the valuation satisfies the target, 0 otherwise\n",
    "    \n",
    "    \"\"\"\n",
    "    if(len(valuation) != len(target)):\n",
    "        print('data length:', str(len(valuation)), ' != target length', str(len(target)))\n",
    "        return 4 # this should never happen, hence and invalid number is given\n",
    "    for i in range (len(target)):\n",
    "        #if t[i] = 3, d[i] = 2 or t[i] == d[i], d entails t\n",
    "        if((target[i] != 3) and (valuation[i] != 2) and (target[i] != valuation[i])):\n",
    "                return 0 # valuation does not satisfy target\n",
    "    return 1 # valuation does satisfy target\n",
    "\n",
    "def categorize_data_w_noise(valuation, target):\n",
    "    \"\"\"\n",
    "    Categorizes valuations based on if they satisfy the target. \n",
    "    With a probability of eta, the categorization will be wrong.\n",
    "    \n",
    "    :param valuation: valuation to categorize\n",
    "    :param target: used classify the valuation\n",
    "    :return : 1 if the valuation satisfies the target, 0 otherwise\n",
    "    \n",
    "    \"\"\"\n",
    "    if(len(valuation) != len(target)):\n",
    "        print('data length:', str(len(valuation)), ' != target length', str(len(target)))\n",
    "        return 4 # this should never happen, hence and invalid number is given\n",
    "    for i in range (len(target)):\n",
    "        #if t[i] = 3, d[i] = 2 or t[i] == d[i], d entails t\n",
    "        misclassify = random.random() <= eta #determine whether classification should be correct or not\n",
    "        if((target[i] != 3) and (valuation[i] != 2) and (target[i] != valuation[i])):\n",
    "            if misclassify:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0 # valuation does not satisfy target\n",
    "    if misclassify:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 # valuation does satisfy target\n",
    "\n",
    "\n",
    "def generate_noisy_examples(data_list, target):\n",
    "    \"\"\"\n",
    "    Generates a list of tupples where each set of valuations is paired with a classification, ie a positive or negative example.\n",
    "    \n",
    "    :param data: set of uncategorized examples\n",
    "    :param data: \n",
    "    :return : list of categorized valuations\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    for valuation in data_list:\n",
    "        category = categorize_data_w_noise(valuation, target)\n",
    "        examples.append((valuation, category))\n",
    "    return examples\n",
    "\n",
    "def disjoint_union(mt, mh):\n",
    "    \"\"\"\n",
    "    Generated the disjoint union between two sets.\n",
    "    \n",
    "    :param mt: set of valuations that satisfy the target\n",
    "    :param mh: det of valuations that satisfy the hypothesis\n",
    "    :return : disjoint union between mt and mh\n",
    "    \"\"\"\n",
    "    for e in mt:\n",
    "        if e in mh:\n",
    "            mt.remove(e)\n",
    "            mh.remove(e)\n",
    "    for e in mh:\n",
    "        if e in mt:\n",
    "            mt.remove(e)\n",
    "            mh.remove(e)\n",
    "    return mt + mh\n",
    "\n",
    "def calculate_t_hat(S, target, hypothesis):\n",
    "    \"\"\"\n",
    "    Calculates the estimated error of the trained hypothesis w.r.t the target. \n",
    "    \n",
    "    :param S: set of unlabeled examples\n",
    "    :param target: to estimate the error w.r.t\n",
    "    :param hypothesis: to estimate the error of\n",
    "    \"\"\"\n",
    "    mt = [] #set of valuations in S that satisfy the target\n",
    "    mh = [] #set of valuations in S that satisfy the target\n",
    "    for s in S:\n",
    "        if (categorize_data(s, target) == 1):\n",
    "            mt.append(s)\n",
    "        if (categorize_data(s, hypothesis) == 1):\n",
    "            mh.append(s)\n",
    "    error_s = disjoint_union(mt, mh) #find misclassified valuations\n",
    "    t_hat = len(error_s) / len(S) # error = (no. misclassified valuations)/(total no. of valuations)\n",
    "    return t_hat\n",
    "\n",
    "\n",
    "def get_all_positive_examples(examples):\n",
    "    \"\"\"\n",
    "    Returns all examples that are categorized as positive:\n",
    "    \n",
    "    :param examples: list of examples to select the positive ones from\n",
    "    :return pos_examples: list of the positively categorized examples\n",
    "    \"\"\"\n",
    "    pos_examples = []\n",
    "    for e in examples:\n",
    "        if e[1] == 1:\n",
    "            pos_examples.append(e)\n",
    "    return pos_examples\n",
    "\n",
    "def translate_binary_literals_to_binary_variables(lit_list):\n",
    "    \"\"\"\n",
    "    Translates a hypotheis representation from literals to hypothesis representation with variables.\n",
    "    \n",
    "    :param lit_list: set of binary representation of literals\n",
    "    : return bin_variables: representation of hypothesis in the form of variables\n",
    "    \"\"\"\n",
    "    bin_variables = np.full(n, 3)\n",
    "    for i in range(len(bin_variables)):\n",
    "        if lit_list[2*(i-1)] == 1:\n",
    "            bin_variables[i] = 1\n",
    "        elif lit_list[2*(i-1)+1] == 1:\n",
    "            bin_variables[i] = 0\n",
    "        else:\n",
    "             bin_variables[i] = 3\n",
    "        #if (lit_list[2*(i-1)] == 1) and (lit_list[2*(i-1)+1] == 1):\n",
    "            #print(\"Error: variable in hypothesis at index\", i, \" is both true and false\")\n",
    "            #print(lit_list)\n",
    "    return bin_variables\n",
    "\n",
    "def get_p0(uncategorized_data):\n",
    "    \"\"\"\n",
    "    Generates an array of p_0(z) for all literals z.\n",
    "    \n",
    "    :param uncategorized_data: set of uncategorized examples.\n",
    "    :return p0: array of p_0(z) for all literals z\n",
    "    \"\"\"\n",
    "    p0_list = np.zeros(2*n)\n",
    "    for e in uncategorized_data:\n",
    "        for i in range(n):\n",
    "            if e[i] == 0:\n",
    "                p0_list[2 * i] += 1\n",
    "            elif e[i] == 1:\n",
    "                 p0_list[(2 * i)+1] += 1\n",
    "            else:\n",
    "                print(\"Error: valuation must be either zero or 1\", e)\n",
    "    p0 = p0_list / k\n",
    "    #print(\"p0\", p0)\n",
    "    return p0\n",
    "\n",
    "def get_set_I(p0):\n",
    "    \"\"\"\n",
    "    Generates the set I.\n",
    "    \n",
    "    :param p0: array of p_0(z) for all literals z\n",
    "    :return set_I: array represenation of the set I\n",
    "    \"\"\"\n",
    "    set_I = np.zeros(2*n)\n",
    "    for i in range(2*n):\n",
    "        if p0[i] >= eps/(32*(2*n)**2):\n",
    "            set_I[i] = 1\n",
    "    return set_I\n",
    "\n",
    "def get_p01(examples):\n",
    "    \"\"\"\n",
    "    Generates an array of p_01(z) for all literals z.\n",
    "    \n",
    "    :param examples: array of categorized examples.\n",
    "    :return p01: array of p_01(z) for all literals z\n",
    "    \"\"\"\n",
    "    pos_ex = get_all_positive_examples(examples)\n",
    "    p01_count = np.zeros(2*n)\n",
    "    for e in pos_ex:\n",
    "        for i in range(n):\n",
    "            if e[0][i] == 0:\n",
    "                p01_count[2 * i] += 1\n",
    "            elif e[0][i] == 1:\n",
    "                 p01_count[(2 * i)+1] += 1\n",
    "    p01 = p01_count / k\n",
    "    return p01\n",
    "\n",
    "def get_apr(p0, p01):\n",
    "    \"\"\"\n",
    "    Generates an array of apr(z) for all literals z.\n",
    "    \n",
    "    :param p0, p01: array of p0(z) and p01(z)\n",
    "    :return apr: array of apr(z) for all literals z. \n",
    "    \"\"\"\n",
    "    apr = np.zeros(2*n)\n",
    "    for i in range(2*n):\n",
    "        apr[i] = p01[i]/p0[i]\n",
    "    return apr\n",
    "\n",
    "def get_number_of_neg_examples(examples):\n",
    "    \"\"\"\n",
    "    Counts the number of negatively classified examples.\n",
    "    \"\"\"\n",
    "    number_of_neg_ex = 0\n",
    "    for e in examples:\n",
    "        if e[1] == 0:\n",
    "            number_of_neg_ex += 1\n",
    "    return number_of_neg_ex\n",
    "\n",
    "def get_noise_estimate(examples, set_I, apr):\n",
    "    \"\"\"\n",
    "    Finds the best estimate for noise, by following the approach in \n",
    "    \"Learning from Noisy Examples\" by Angluin et al.\n",
    "    \n",
    "    :param examples: array of categorized examples.\n",
    "    :param set_I: the set of significant literals\n",
    "    :return final_noise_estimate: best estimate for noise\n",
    "    \"\"\"\n",
    "    number_of_neg_examples = get_number_of_neg_examples(examples)\n",
    "    noise_estimate_1 = number_of_neg_examples/k\n",
    "    noise_estimate_2 = 1\n",
    "    for i in range(2*n):\n",
    "        if set_I[i] == 1:\n",
    "            if apr[i] < noise_estimate_2:\n",
    "                noise_estimate_2 = apr[i]\n",
    "    final_noise_estimate = min(noise_estimate_1, noise_estimate_2)\n",
    "    return final_noise_estimate\n",
    "\n",
    "def generate_hypothsis_from_noisy_examples(set_I, apr, noise_estimate):\n",
    "    \"\"\"\n",
    "    Using the algorithm described in \"Learning from Noisy Examples\" by Angluin et al.\n",
    "    the method trains up a hypothesis with noisy examples.\n",
    "    \n",
    "    :param set_I: the set of significant literals\n",
    "    :param apr: array with apr(z) for each literal z\n",
    "    :param  noise_estimate: estimate for noise\n",
    "    \"\"\"\n",
    "    final_hyp_literals = np.zeros(2*n)\n",
    "    for i in range(2*n):\n",
    "        if set_I[i] == 1: #if the literal is in I\n",
    "            if apr[i] <= (noise_estimate + (eps * (1 - 2 * eta) / (4 * (2*n)))):\n",
    "                final_hyp_literals[i] = 1\n",
    "    final_hyp = translate_binary_literals_to_binary_variables(final_hyp_literals)\n",
    "    return final_hyp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAC implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables used: 10\n",
      "Epsilon: 0.4\n",
      "Delta: 0.1\n",
      "Eta_b: 0.01\n",
      "k: 1000\n",
      "\n",
      "Normal distribution used for generating training set.\n",
      "Target: [3, 3, 0, 3, 3, 0, 3, 3, 3, 3]\n",
      "Trained hyp: [3 3 3 0 3 3 0 3 3 3]\n",
      "Error: 0.378925\n"
     ]
    }
   ],
   "source": [
    "# Print out arguments being used\n",
    "print('Number of variables used:', n)\n",
    "print('Epsilon:', eps)\n",
    "print('Delta:', delta)\n",
    "print('Eta_b:', eta)\n",
    "print('k:', k)\n",
    "print()\n",
    "\n",
    "target = generate_satisfiable_target(2,n) # we generate a target with 2 bases\n",
    "uncategorized_data = generate_uncategorized_data(k, n)\n",
    "examples = generate_noisy_examples(uncategorized_data, target)\n",
    "#calculate p0(z) for all literals\n",
    "p0 = get_p0(uncategorized_data)\n",
    "#Generate the set I\n",
    "set_I = get_set_I(p0)\n",
    "#Calculate p01 for all literals\n",
    "p01 = get_p01(examples)\n",
    "#Calculate apr(z) for all literals\n",
    "apr = get_apr(p0, p01)\n",
    "#Get estimate for noise\n",
    "noise_estimate = get_noise_estimate(examples, set_I, apr)\n",
    "trained_hypothesis = generate_hypothsis_from_noisy_examples(set_I, apr, noise_estimate)\n",
    "S = generate_uncategorized_data(N, n)\n",
    "t = calculate_t_hat(S, target, trained_hypothesis) #calculate the error\n",
    "print(\"Normal distribution used for generating training set.\")\n",
    "print(\"Target:\", target)\n",
    "print(\"Trained hyp:\", trained_hypothesis)\n",
    "print(\"Error:\", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization guarantee\n",
    "Through a bit of trial and error, I ended up choosing eta_b = 0.01 and k = 4200 for my algorithm.\n",
    "\n",
    "Unfortunately the runtime for the large k required for epsilon = 0.3 on my computer was 30min+. I let it run for a couple of different values of k and got these results:\n",
    "\n",
    "| k      | bad error rate |\n",
    "| ----------- | ----------- |\n",
    "| 1000      | 0.35|\n",
    "| 2000      | 0.17 |\n",
    "| 3000      | 0.13|\n",
    "| 4000      | 0.1    |\n",
    "| 4100      | 0.8   |\n",
    "| 4200      | 0.5   |\n",
    "| 4300   | 0.7  |\n",
    "| 4500   | 0.12  |\n",
    "\n",
    "So it seemed 4200 is a good value for k in this implementation of the algorithm.\n",
    "\n",
    "I have now changed epsilon to 0.04, where I found that k = 1000 is enough to get a bad error rate less than or equal to 0.1. It still takes a couple of minutes to run for me, but hopefully your computer is faster than mine and it will only take a few seconds of your time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables used: 10\n",
      "Epsilon: 0.4\n",
      "Delta: 0.1\n",
      "Eta_b: 0.01\n",
      "k: 1000\n",
      "10/delta: 100\n",
      "N: 40000\n",
      "\n",
      "Number of iterations: 100\n",
      "Currently on interation 0\n",
      "Currently on interation 1\n",
      "Currently on interation 2\n",
      "Currently on interation 3\n",
      "Currently on interation 4\n",
      "Currently on interation 5\n",
      "Currently on interation 6\n",
      "Currently on interation 7\n",
      "Currently on interation 8\n",
      "Currently on interation 9\n",
      "Currently on interation 10\n",
      "Currently on interation 11\n",
      "Currently on interation 12\n",
      "Currently on interation 13\n",
      "Currently on interation 14\n",
      "Currently on interation 15\n",
      "Currently on interation 16\n",
      "Currently on interation 17\n",
      "Currently on interation 18\n",
      "Currently on interation 19\n",
      "Currently on interation 20\n",
      "Currently on interation 21\n",
      "Currently on interation 22\n",
      "Currently on interation 23\n",
      "Currently on interation 24\n",
      "Currently on interation 25\n",
      "Currently on interation 26\n",
      "Currently on interation 27\n",
      "Currently on interation 28\n",
      "Currently on interation 29\n",
      "Currently on interation 30\n",
      "Currently on interation 31\n",
      "Currently on interation 32\n",
      "Currently on interation 33\n",
      "Currently on interation 34\n",
      "Currently on interation 35\n",
      "Currently on interation 36\n",
      "Currently on interation 37\n",
      "Currently on interation 38\n",
      "Currently on interation 39\n",
      "Currently on interation 40\n",
      "Currently on interation 41\n",
      "Currently on interation 42\n",
      "Currently on interation 43\n",
      "Currently on interation 44\n",
      "Currently on interation 45\n",
      "Currently on interation 46\n",
      "Currently on interation 47\n",
      "Currently on interation 48\n",
      "Currently on interation 49\n",
      "Currently on interation 50\n",
      "Currently on interation 51\n",
      "Currently on interation 52\n",
      "Currently on interation 53\n",
      "Currently on interation 54\n",
      "Currently on interation 55\n",
      "Currently on interation 56\n",
      "Currently on interation 57\n",
      "Currently on interation 58\n",
      "Currently on interation 59\n",
      "Currently on interation 60\n",
      "Currently on interation 61\n",
      "Currently on interation 62\n",
      "Currently on interation 63\n",
      "Currently on interation 64\n",
      "Currently on interation 65\n",
      "Currently on interation 66\n",
      "Currently on interation 67\n",
      "Currently on interation 68\n",
      "Currently on interation 69\n",
      "Currently on interation 70\n",
      "Currently on interation 71\n",
      "Currently on interation 72\n",
      "Currently on interation 73\n",
      "Currently on interation 74\n",
      "Currently on interation 75\n",
      "Currently on interation 76\n",
      "Currently on interation 77\n",
      "Currently on interation 78\n",
      "Currently on interation 79\n",
      "Currently on interation 80\n",
      "Currently on interation 81\n",
      "Currently on interation 82\n",
      "Currently on interation 83\n",
      "Currently on interation 84\n",
      "Currently on interation 85\n",
      "Currently on interation 86\n",
      "Currently on interation 87\n",
      "Currently on interation 88\n",
      "Currently on interation 89\n",
      "Currently on interation 90\n",
      "Currently on interation 91\n",
      "Currently on interation 92\n",
      "Currently on interation 93\n",
      "Currently on interation 94\n",
      "Currently on interation 95\n",
      "Currently on interation 96\n",
      "Currently on interation 97\n",
      "Currently on interation 98\n",
      "Currently on interation 99\n",
      "Number of errors greater than epsilon: 7\n",
      "Bad error rate: 0.07\n",
      "Runtime for k 1000  is: 171.70323519999988 seconds\n"
     ]
    }
   ],
   "source": [
    "# Print out arguments being used\n",
    "print('Number of variables used:', n)\n",
    "print('Epsilon:', eps)\n",
    "print('Delta:', delta)\n",
    "print('Eta_b:', eta)\n",
    "print('k:', k)\n",
    "print('10/delta:', ten_div_delta)\n",
    "print('N:', N)\n",
    "print()\n",
    "\n",
    "print(\"Number of iterations:\", ten_div_delta)\n",
    "bad_error_count = 0 # number of errors larger than epsilon\n",
    "start = timeit.default_timer()\n",
    "for i in range(ten_div_delta):\n",
    "    print(\"Currently on interation\", i)\n",
    "    target = generate_satisfiable_target(2,n) # we generate a target with 2 bases\n",
    "    uncategorized_data = generate_uncategorized_data(k, n)\n",
    "    examples = generate_noisy_examples(uncategorized_data, target)\n",
    "    #calculate p0(z) for all literals\n",
    "    p0 = get_p0(uncategorized_data)\n",
    "    #Generate the set I\n",
    "    set_I = get_set_I(p0)\n",
    "    #Calculate p01 for all literals\n",
    "    p01 = get_p01(examples)\n",
    "    #Calculate apr(z) for all literals\n",
    "    apr = get_apr(p0, p01)\n",
    "    #Get estimate for noise\n",
    "    noise_estimate = get_noise_estimate(examples, set_I, apr)\n",
    "    trained_hypothesis = generate_hypothsis_from_noisy_examples(set_I, apr, noise_estimate)\n",
    "    S = generate_uncategorized_data(N, n)\n",
    "    t = calculate_t_hat(S, target, trained_hypothesis) #calculate the error\n",
    "    if (t > eps):\n",
    "        bad_error_count = bad_error_count + 1 # count the number of errors greater than epsilon\n",
    "print('Number of errors greater than epsilon:', bad_error_count)\n",
    "print('Bad error rate:', bad_error_count/ten_div_delta)\n",
    "stop = timeit.default_timer()\n",
    "print('Runtime for k', k, \" is:\", stop - start, \"seconds\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
